{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('这是', 'VERB'), ('一个', 'ADV'), ('用于', 'VERB'), ('示例', 'NOUN'), ('的', 'PART'), ('句子', 'NOUN'), ('。', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"这是一个用于示例的句子。\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./testfile/中文语料.txt\") as f:\n",
    "    chinesetext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chinesetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(\n",
    "        separator = \"\\n\\n\",\n",
    "        pipeline = \"zh_core_web_sm\",  # 加载 spacy 的中文模型\n",
    "        max_length = 1_000_000,\n",
    "        chunk_size=1000,    # 最大切割字符：1000个\n",
    "        chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(chinesetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(texts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967\n",
      "896\n",
      "965\n",
      "992\n",
      "994\n",
      "986\n",
      "922\n",
      "976\n",
      "962\n",
      "971\n",
      "969\n",
      "983\n",
      "973\n",
      "974\n",
      "927\n",
      "426\n"
     ]
    }
   ],
   "source": [
    "for _ in texts:\n",
    "    print(len(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入到 flowcontrol\n",
    "# 3.分割文档 spliter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "text_splitter = SpacyTextSplitter(\n",
    "        separator = \"\\n\\n\",\n",
    "        pipeline = \"zh_core_web_sm\",  # 加载 spacy 的中文模型\n",
    "        max_length = 1_000_000,\n",
    "        chunk_size=1000,    # 最大切割字符：1000个\n",
    "        chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      5\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m SpacyTextSplitter(\n\u001b[1;32m      6\u001b[0m         separator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 加载 spacy 的中文模型\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1_000_000\u001b[39m,\n\u001b[1;32m      9\u001b[0m         chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,    \u001b[38;5;66;03m# 最大切割字符：1000个\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         chunk_overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m texts_splited \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# texts_splited -> list[str]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 构造 langchain Document 对象\u001b[39;00m\n\u001b[1;32m     16\u001b[0m text_Document_list \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename or book.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m}) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts_splited]\n",
      "File \u001b[0;32m/opt/conda/envs/DocDoc/lib/python3.10/site-packages/langchain/text_splitter.py:1410\u001b[0m, in \u001b[0;36mSpacyTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split incoming text and return chunks.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1410\u001b[0m     splits \u001b[38;5;241m=\u001b[39m (s\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msents)\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_splits(splits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator)\n",
      "File \u001b[0;32m/opt/conda/envs/DocDoc/lib/python3.10/site-packages/spacy/language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1018\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/envs/DocDoc/lib/python3.10/site-packages/spacy/language.py:1131\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n\u001b[0;32m-> 1131\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1041\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'list'>"
     ]
    }
   ],
   "source": [
    "# 加入到 flowcontrol\n",
    "# 3.分割文档 spliter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "text_splitter = SpacyTextSplitter(\n",
    "        separator = \"\\n\\n\",\n",
    "        pipeline = \"zh_core_web_sm\",  # 加载 spacy 的中文模型\n",
    "        max_length = 1_000_000,\n",
    "        chunk_size=1000,    # 最大切割字符：1000个\n",
    "        chunk_overlap = 200)\n",
    " \n",
    "texts_splited = text_splitter.split_text(texts)\n",
    "# texts_splited -> list[str]\n",
    "\n",
    "# 构造 langchain Document 对象\n",
    "text_Document_list = [Document(page_content=text, metadata={\"from\": \"filename or book.txt\"}) for text in texts_splited]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocDoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
