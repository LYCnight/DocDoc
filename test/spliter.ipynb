{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n",
      "\n",
      "Members of Congress and the Cabinet.\n",
      "\n",
      "Justices of the Supreme Court.\n",
      "\n",
      "My fellow Americans.\n",
      "\n",
      "Last year COVID-19 kept us apart.\n",
      "\n",
      "This year we are finally together again.\n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents.\n",
      "\n",
      "But most importantly as Americans.\n",
      "\n",
      "With a duty to one another to the American people to the Constitution.\n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny.\n",
      "\n",
      "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\n",
      "\n",
      "But he badly miscalculated.\n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over.\n",
      "\n",
      "Instead he met a wall of strength he never imagined.\n",
      "\n",
      "He met the Ukrainian people.\n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n",
      "\n",
      "Groups of citizens blocking tanks with their bodies.\n"
     ]
    }
   ],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./testfile/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000) # How the chunk size is measured: by number of characters.\n",
    "\n",
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./testfile/中文语料.txt\") as f:\n",
    "    Chinesetext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2569, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text2 = text_splitter.split_text(Chinesetext)\n",
    "print(type(text2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高级检索增强生成技术(RAG)全面指南：原理、分块、编码、索引、微调、Agent、展望\n",
      "\n",
      "\n",
      "ChatGPT、Midjourney等生成式人工智能（GenAI）在文本生成、文本到图像生成等任务中表现出令人印象深刻的性能。然而，生成模型也不能避免其固有的局限性，包括产生幻觉的倾向，在数学能力弱，而且缺乏可解释性。因此，提高他们能力的一个可行办法是让他们能够与外部世界互动，以不同的形式和方式获取知识，从而提高所生成内容的事实性和合理性。\n",
      "检索增强生成（Retrieval-Augmented Generation, RAG）技术研究旨在提供更有依据、更依赖事实的信息来帮助解决生成式AI的幻觉倾向、专业力弱等固有缺陷。RAG最新科研综述可以参考：万字综述：2023年多模态检索增强生成技术(mRAG)最新进展与趋势-图片、代码、图谱、视频、声音、文本。\n",
      "\n",
      "现在已有大多数教程都挑选一种或几种RAG技术并详细解释如何实现它们，而缺少一个对于RAG技术的系统化概述。本文的目的是想系统化梳理关键的高级RAG技术，并介绍它们的实现，以便于其他开发人员深入研究该技术。\n",
      "\n",
      "\n",
      "1 - 检索增强生成(RAG)技术简介\n",
      "\n",
      "检索增强生成（又名RAG）为大语言模型提供从某些数据源检索到的信息，作为其生成答案的依据。RAG通常包括两个阶段：检索上下文相关信息和使用检索到的知识指导生成过程。基本上，RAG是通过检索算法找到的信息作为上下文，帮助大模型回答用户问询。查询和检索到的上下文都被注入到发送给LLM的提示中。\n",
      "\n",
      "RAG是2023年最流行的基于LLM的系统架构。有许多产品几乎完全基于RAG构建-从将网络搜索引擎与LLM相结合的问答服务到数百个与数据交互的应用程序。\n",
      "\n",
      "矢量搜索领域也受到了RAG火热的推动而进一步提升了重要度。像chroma, weavaite.io, pinecone这样的矢量数据库初创公司都是建立在现有的开源搜索索引（主要是faiss和nmslib）的基础上的最近为输入文本添加了额外的存储空间以及一些其他工具。\n",
      "\n",
      "基于大模型的的应用程序有多个最著名的开源库，包括LangChain，LlamaIndex，AutoGen, PromptAppGPT等。对于LLM应用开发可以参考如下文章进一步了解：\n",
      "\n",
      "2023年人工智能体(AI Agent)开发与应用全面调研：概念、原理、开发、应用、挑战、展望\n",
      "\n",
      "PromptAppGPT低代码大语言模型应用开发实战：聊天机器人、智能文生图、图文搜索\n",
      "\n",
      "\n",
      "2 - 基础检索增强生成(RAG)技术\n",
      "基础RAG流程简单来说如下：将文本分割成块，然后使用编码模型将这些块嵌入到向量中，将所有这些向量放入索引中，最后为LLM创建一个提示，告诉模型根据我们在搜索步骤中找到的上下文来回答用户的查询。\n",
      "\n",
      "在运行时，我们使用相同的编码器模型对用户的查询进行矢量化，然后针对索引执行该查询向量的搜索，找到前k个结果，从数据库中检索相应的文本块，并将它们作为上下文输入到LLM提示中。\n",
      "\n",
      "提示工程是可以尝试改进RAG系统的最便捷便宜的方法。可以参考如下文章了解更多提示工程技巧：\n",
      "\n",
      "从1000+模板中总结出的10大提示工程方法助你成为提示词大师！\n",
      "\n",
      "ChatGPT提示工程师必备基础知识与进阶技巧！\n",
      "\n",
      "显然，尽管OpenAI是LLM提供商的市场领导者，但仍有许多替代方案，例如Anthropic的Claude、最近流行的较小但功能强大的模型，例如Mixtral形式的Mistral、Microsoft的Phi-2以及许多开源选项，例如Llama2、OpenLLaMA、Falcon。\n",
      "\n",
      "\n",
      "3 - 高级检索增强生成(RAG)技术\n",
      "\n",
      "现在我们将深入了解高级RAG技术的核心步骤和所涉及算法。为了保持方案的可读性，我们省略了一些逻辑循环和复杂的多步骤代理行为。\n",
      "\n",
      "方案图中的绿色元素是进一步讨论的核心RAG技术，蓝色元素是文本。并非所有先进的RAG技术都可以在单个方案中轻松可视化，例如，省略了各种上下文放大方法，不过，我们将在文中深入探讨这些技术。\n",
      "\n",
      "3.1 - 分块和矢量化\n",
      "\n",
      "首先，我们要创建一个向量索引，表示我们的文档内容，然后在运行时搜索所有这些向量与对应于最接近语义的查询向量之间的最小余弦距离。\n",
      "\n",
      "3.1.1 - 分块\n",
      "\n",
      "Transformer模型具有固定的输入序列长度，即使输入上下文窗口很大，一个或几个句子的向量也比几页文本的平均向量更好地表示其语义，因此需要对数据进行分块。分块将初始文档分割成一定大小的块，尽量不要失去语义含义，将文本分割成句子或段落，而不是将单个句子分成两部分。有多种文本分割器实现能够完成此任务。\n",
      "\n",
      "块的大小是一个需要考虑的重要参数-它取决于您使用的嵌入模型及其令牌容量，标准转换器编码器模型（例如基于BERT的句子转换器）最多需要512个令牌。OpenAI ada-002能够处理更长的序列，如8191个标记，但这里就需要权衡是留有足够的上下文供大语言模型进行推理，还是留足够具体的文本表征以便有效地执行检索。\n",
      "\n",
      "例如，在语义搜索中，我们对文档语料库进行索引，每个文档都包含有关特定主题的有价值的信息。通过应用有效的分块策略，我们可以确保我们的搜索结果准确地捕捉用户查询的本质。如果我们的块太小或太大，可能会导致搜索结果不精确或错过显示相关内容的机会。根据经验，如果文本块在没有周围上下文的情况下对人类有意义，那么它对语言模型也有意义。因此，找到语料库中文档的最佳块大小对于确保搜索结果的准确性和相关性至关重要。\n",
      "\n",
      "另一个例子是会话Agent。我们使用嵌入的块根据知识库为会话Agent构建上下文，该知识库使Agent基于可信信息。在这种情况下，对我们的分块策略做出正确的选择很重要，原因有两个：首先，它将确定上下文是否确实与我们的提示相关。其次，考虑到我们可以为每个请求发送的令牌数量的限制，它将确定我们是否能够在将检索到的文本发送到外部模型提供商（例如 OpenAI）之前将其放入上下文中。在某些情况下，例如在 32k 上下文窗口中使用 GPT-4 时，拟合块可能不是问题。尽管如此，我们需要注意何时使用非常大的块，因为这可能会对检索结果的相关性产生不利影响。\n",
      "\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "print(text2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2569\n",
      "9337\n"
     ]
    }
   ],
   "source": [
    "for _ in text2:\n",
    "    print(len(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./testfile/state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DocDoc/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\n",
      "\n",
      "Members of Congress and the Cabinet.\n",
      "\n",
      "Justices of the Supreme Court.\n",
      "\n",
      "My fellow Americans.\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Last year COVID-19 kept us apart.\n",
      "\n",
      "This year we are finally together again.\n",
      "\n",
      "\n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents.\n",
      "\n",
      "But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\n",
      "\n",
      "But he badly miscalculated. \n",
      "\n",
      "\n",
      "\n",
      "He thought he could roll into Ukraine and the world would roll over.\n",
      "\n",
      "Instead he met a wall of strength he never imagined.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "He met the Ukrainian people.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DocDoc/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  oov_forms = []\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(state_of_the_union)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.\\n\\nMembers of Congress and the Cabinet.\\n\\nJustices of the Supreme Court.\\n\\nMy fellow Americans.\\n\\n \\n\\n\\n\\nLast year COVID-19 kept us apart.\\n\\nThis year we are finally together again.\\n\\n\\n\\nTonight, we meet as Democrats Republicans and Independents.\\n\\nBut most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution.\\n\\n\\n\\n\\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny.\\n\\n\\n\\n\\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.\\n\\nBut he badly miscalculated. \\n\\n\\n\\nHe thought he could roll into Ukraine and the world would roll over.\\n\\nInstead he met a wall of strength he never imagined.\\n\\n\\n\\n\\n\\nHe met the Ukrainian people.\\n\\n\\n\\n\\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"你好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n",
      "932\n",
      "981\n",
      "948\n",
      "981\n",
      "985\n",
      "970\n",
      "983\n",
      "967\n",
      "975\n",
      "913\n",
      "975\n",
      "989\n",
      "984\n",
      "869\n",
      "830\n",
      "986\n",
      "867\n",
      "927\n",
      "990\n",
      "890\n",
      "697\n",
      "974\n",
      "912\n",
      "923\n",
      "915\n",
      "932\n",
      "846\n",
      "946\n",
      "922\n",
      "884\n",
      "966\n",
      "990\n",
      "924\n",
      "991\n",
      "858\n",
      "902\n",
      "934\n",
      "955\n",
      "933\n",
      "919\n",
      "961\n",
      "989\n",
      "962\n",
      "965\n",
      "898\n",
      "893\n",
      "996\n",
      "905\n",
      "828\n"
     ]
    }
   ],
   "source": [
    "for _ in texts:\n",
    "    print(len(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open(\"./testfile/中文语料.txt\") as f:\n",
    "    chinesetext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DocDoc/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.0.0) was trained with spaCy v3.0.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/DocDoc/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  oov_forms = []\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(chinesetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487\n",
      "1022\n",
      "274\n",
      "973\n",
      "921\n",
      "811\n",
      "746\n",
      "820\n",
      "782\n",
      "767\n",
      "942\n",
      "945\n",
      "915\n",
      "675\n",
      "428\n",
      "993\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "for _ in texts:\n",
    "    print(len(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高级检索增强生成技术(RAG)全面指南：原理、分块、编码、索引、微调、Agent、展望\n",
      "\n",
      "\n",
      "ChatGPT、Midjourney等生成式人工智能（GenAI）在文本生成、文本到图像生成等任务中表现出令人印象深刻的性能。然而，生成模型也不能避免其固有的局限性，包括产生幻觉的倾向，在数学能力弱，而且缺乏可解释性。因此，提高他们能力的一个可行办法是让他们能够与外部世界互动，以不同的形式和方式获取知识，从而提高所生成内容的事实性和合理性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "检索增强生成（Retrieval-Augmented Generation, RAG）技术研究旨在提供更有依据、更依赖事实的信息来帮助解决生成式AI的幻觉倾向、专业力弱等固有缺陷。RAG最新科研综述可以参考：万字综述：2023年多模态检索增强生成技术(mRAG)最新进展与趋势-图片、代码、图谱、视频、声音、文本。\n",
      "\n",
      "\n",
      "\n",
      "现在已有大多数教程都挑选一种或几种RAG技术并详细解释如何实现它们，而缺少一个对于RAG技术的系统化概述。本文的目的是想系统化梳理关键的高级RAG技术，并介绍它们的实现，以便于其他开发人员深入研究该技术\n",
      "\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - 检索增强生成(RAG)技术简介\n",
      "\n",
      "检索增强生成（又名RAG）为大语言模型提供从某些数据源检索到的信息，作为其生成答案的依据。RAG通常包括两个阶段：检索上下文相关信息和使用检索到的知识指导生成过程。基本上，RAG是通过检索算法找到的信息作为上下文，帮助大模型回答用户问询。查询和检索到的上下文都被注入到发送给LLM的提示中。\n",
      "\n",
      "RAG是2023年最流行的基于LLM的系统架构。有许多产品几乎完全基于RAG构建-从将网络搜索引擎与LLM相结合的问答服务到数百个与数据交互的应用程序。\n",
      "\n",
      "矢量搜索领域也受到了RAG火热的推动而进一步提升了重要度。像chroma, weavaite.io, pinecone这样的矢量数据库初创公司都是建立在现有的开源搜索索引（主要是faiss和nmslib）的基础上的最近为输入文本添加了额外的存储空间以及一些其他工具。\n",
      "\n",
      "基于大模型的的应用程序有多个最著名的开源库，包括LangChain，LlamaIndex，AutoGen, PromptAppGPT等。对于LLM应用开发可以参考如下文章进一步了解：\n",
      "\n",
      "2023年人工智能体(AI Agent)开发与应用全面调研：概念、原理、开发、应用、挑战、展望\n",
      "\n",
      "PromptAppGPT低代码大语言模型应用开发实战：聊天机器人、智能文生图、图文搜索\n",
      "\n",
      "\n",
      "2 - 基础检索增强生成(RAG)技术\n",
      "基础RAG流程简单来说如下：将文本分割成块，然后使用编码模型将这些块嵌入到向量中，将所有这些向量放入索引中，最后为LLM创建一个提示，告诉模型根据我们在搜索步骤中找到的上下文来回答用户的查询。\n",
      "\n",
      "在运行时，我们使用相同的编码器模型对用户的查询进行矢量化，然后针对索引执行该查询向量的搜索，找到前k个结果，从数据库中检索相应的文本块，并将它们作为上下文输入到LLM提示中。\n",
      "\n",
      "提示工程是可以尝试改进RAG系统的最便捷便宜的方法。可以参考如下文章了解更多提示工程技巧：\n",
      "\n",
      "从1000+模板中总结出的10大提示工程方法助你成为提示词大师！\n",
      "\n",
      "ChatGPT提示工程师必备基础知识与进阶技巧！\n",
      "\n",
      "显然，尽管OpenAI是LLM提供商的市场领导者，但仍有许多替代方案，例如Anthropic的Claude、最近流行的较小但功能强大的模型，例如Mixtral形式的Mistral、Microsoft的Phi-2以及许多开源选项，例如Llama2、OpenLLaMA、Falcon\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DocDoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
